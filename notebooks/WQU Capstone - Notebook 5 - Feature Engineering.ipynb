{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WQU Capstone project - Short-term trading strategy on G10 currencies\n",
    "## Notebook five - Putting previous steps together and Feature Engineering\n",
    "\n",
    "* Sergey Chigrinov - chigrinov.s.88@gmail.com\n",
    "* Dhruv Agraval -  dhruva1@stanfordalumni.org\n",
    "* Man Sing Ho - mshoalbert@gmail.com\n",
    "\n",
    "### Jun-Aug-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these steps the data should be ready for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "#insert you own path or use relative path\n",
    "path_to_project = os.path.realpath('..') # r'C:\\WQU\\Capstone\\Working_files'\n",
    "sys.path.append(path_to_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from pandas.tseries.offsets import BMonthEnd\n",
    "from multiprocessing import cpu_count\n",
    "#from statsmodels.tsa.stattools import adfuller\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WQUcapstoneCode.sampling import sampling\n",
    "from WQUcapstoneCode.labeling import labeling\n",
    "from WQUcapstoneCode.technical import technical\n",
    "from WQUcapstoneCode.fracdif.fracdif import frac_diff_ffd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['AUD/USD','AUD/CAD','AUD/JPY','EUR/USD','GBP/USD','NZD/USD','USD/CAD', 'USD/JPY']\n",
    "max_holding_period = 999 #days \n",
    "ticks_multiplyer = 1. #arbitrary\n",
    "min_ret_target_vol_multiplier = 0.7 #\n",
    "cpus = cpu_count()-1\n",
    "d = 0.35 #fracdiff parameter. 1=simple first order differencing\n",
    "#we can use ADF to find non-stationary features, however, \n",
    "#fractionally differentiated featured may hold useful information for further analysis\n",
    "#therefore, we'll use default list of features to apply fracDiff\n",
    "non_stationary_feat = {'price','fast','slow','average','upper_band','lower_band','tenka_sen','kijun_sen','senkou_span_a','senkou_span_b'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 19050/19050 [00:00<00:00, 489619.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 11722/11722 [00:03<00:00, 3618.58it/s]\n",
      "2020-06-28 18:59:53.229695 100.0% applyPtSlOnT1 done after 0.06 minutes. Remaining 0.0 minutes..\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:21<00:00,  2.17s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 19027/19027 [00:00<00:00, 681342.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 11500/11500 [00:02<00:00, 4656.20it/s]\n",
      "2020-06-28 19:00:58.918743 100.0% applyPtSlOnT1 done after 0.05 minutes. Remaining 0.0 minutes..\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:23<00:00,  2.33s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 19056/19056 [00:00<00:00, 1469888.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 11570/11570 [00:02<00:00, 3953.97it/s]\n",
      "2020-06-28 19:01:55.897957 100.0% applyPtSlOnT1 done after 0.06 minutes. Remaining 0.0 minutes..\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:29<00:00,  2.91s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 19066/19066 [00:00<00:00, 976656.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 11090/11090 [00:02<00:00, 4683.14it/s]\n",
      "2020-06-28 19:03:04.218068 100.0% applyPtSlOnT1 done after 0.04 minutes. Remaining 0.0 minutes..\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:44<00:00,  4.49s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 19031/19031 [00:00<00:00, 706762.88it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 11346/11346 [00:03<00:00, 3780.77it/s]\n",
      "2020-06-28 19:04:38.733688 100.0% applyPtSlOnT1 done after 0.05 minutes. Remaining 0.0 minutes..\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:21<00:00,  2.18s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 19026/19026 [00:00<00:00, 681382.80it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 11630/11630 [00:02<00:00, 4638.47it/s]\n",
      "2020-06-28 19:05:37.917268 100.0% applyPtSlOnT1 done after 0.05 minutes. Remaining 0.0 minutes..\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:21<00:00,  2.16s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 19040/19040 [00:00<00:00, 596585.62it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 11020/11020 [00:02<00:00, 4548.98it/s]\n",
      "2020-06-28 19:06:33.230383 100.0% applyPtSlOnT1 done after 0.04 minutes. Remaining 0.0 minutes..\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:24<00:00,  2.45s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 19058/19058 [00:00<00:00, 364026.15it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 11329/11329 [00:04<00:00, 2751.10it/s]\n",
      "2020-06-28 19:07:32.079536 100.0% applyPtSlOnT1 done after 0.04 minutes. Remaining 0.0 minutes..\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:22<00:00,  2.29s/it]\n"
     ]
    }
   ],
   "source": [
    "offset = BMonthEnd()\n",
    "for ticker in tickers:\n",
    "    input_path = os.path.join(path_to_project, 'input_data', ''.join(ticker.split('/')) + '.csv')\n",
    "    pair = pd.read_csv(input_path)\n",
    "    pair.index =[dt.datetime.strptime(date, '%Y-%m-%d %H:%M:%S') for date in pair.date]\n",
    "    pair = pair.drop(columns=['date'])\n",
    "    m_ticks = ticks_multiplyer * pair.tickqty.sum()/pair.shape[0]\n",
    "    #print('Sampling')\n",
    "    tick_df = sampling.sampled_bar_df(pair, 'tickqty',m_ticks)\n",
    "    \n",
    "    #print('Labeling')   \n",
    "    dailyVol = labeling.getDailyVol(tick_df.bidclose) \n",
    "    dailyVol.name = 'volatility'\n",
    "    close = tick_df[['bidclose','askclose']]\n",
    "    close.columns = ['bid','ask']  \n",
    "    tEvents = labeling.getTEvents(close,h=dailyVol.mean())\n",
    "    t1 = labeling.addVerticalBarrier(tEvents, close, numDays=max_holding_period)\n",
    "    ptsl = [1, 1] #symmetric take-profit and stop-loss\n",
    "    target = dailyVol\n",
    "    # select minRet\n",
    "    minRet = dailyVol.mean()*min_ret_target_vol_multiplier\n",
    "    close = (close['bid'] + close['ask']) / 2 #to simplify we'll work with the mid price\n",
    "    events = labeling.getEvents(close, tEvents, ptsl, target, minRet, cpus, t1=t1)\n",
    "    labels = labeling.getBins(events, close)\n",
    "    labels = labeling.dropLabels(labels)\n",
    "    \n",
    "    #print('Calculating technical features')\n",
    "    ema = technical.EMA(close)\n",
    "    bb = technical.BollingerBands(close)\n",
    "    cci = technical.CCI(close)\n",
    "    so = technical.Stochastic(close)\n",
    "    wr = technical.wr(close)\n",
    "    ic = technical.Ichimoku(close)\n",
    "    rsi = technical.RSI(close)\n",
    "    \n",
    "    lagged_px = pd.concat([close.shift(1), close.shift(2),close.shift(12)],axis=1)\n",
    "    lagged_px.columns = ['T-1','T-2','T-12']\n",
    "    lagged_returns = pd.DataFrame(np.diff(np.log(lagged_px), axis=0), \n",
    "                                  index = lagged_px.index[1:], \n",
    "                                  columns=[c+'_1per_rtn' for c in lagged_px])\n",
    "    period_returns = pd.concat([np.log(close/close.shift(1)), \n",
    "                                np.log(close/close.shift(1)), \n",
    "                                np.log(close/close.shift(1))],axis=1)\n",
    "    period_returns.columns = ['T-1_rtn','T-2_rtn','T-12_rtn']\n",
    "    lags = pd.concat([technical.rolling_autocorr(close,lag=1),\n",
    "                      technical.rolling_autocorr(close,lag=2),\n",
    "                      technical.rolling_autocorr(close,lag=4),\n",
    "                      technical.rolling_autocorr(close,lag=6)], axis=1) #2,4,6 were found to be correlated earlier\n",
    "    \n",
    "    ema.data = ema.data.rename(columns={'side':'ema_side'})\n",
    "    bb.data = bb.data.rename(columns={'side':'bb_side'})\n",
    "    so.data = so.data.rename(columns={'side':'so_side'})\n",
    "    cci.data = cci.data.rename(columns={'side':'cci_side'})\n",
    "    wr.data = cci.data.rename(columns={'side':'wr_side'})\n",
    "    ic.data = ic.data.rename(columns={'side':'ic_side'})\n",
    "    rsi.data = rsi.data.rename(columns={'side':'rsi_side'})\n",
    "    feat = lambda x: [col for col in x.columns if col != 'price'] \n",
    "    features = pd.concat([ema(), bb()[feat], so()[feat], cci()[feat],wr()[feat],\n",
    "                          ic()[feat], rsi()[feat], lagged_px, lags,\n",
    "                          lagged_returns, period_returns], axis=1)\n",
    "    #day of the week and month may be useful features as well because of rebalancing flows\n",
    "    #we one-hot-encode them\n",
    "    features = pd.concat([features,\n",
    "                          pd.get_dummies(features.index.day_name()).set_index(features.index),\n",
    "                          pd.get_dummies(features.index.month_name()).set_index(features.index)], axis=1)\n",
    "    #features['day'], features['month'] = features.index.dayofweek, features.index.month\n",
    "    features['EOM']=features.index.map(lambda x: 1 if (offset.rollforward(x).day==x.day) else 0)\n",
    "    #features['EOQ']=features.index.map(lambda x: 1 if ((offset.rollforward(x).day==x.day) and (x.month in (3,6,9,12))) else 0)\n",
    "    #print('Applying fracDiff')\n",
    "    df = pd.concat([frac_diff_ffd(pd.DataFrame(features[c].dropna()), diff_amt=d, thresh=1e-5)  for c in tqdm(non_stationary_feat)], axis = 1)\n",
    "    df.columns = [f'{c}_frdif' for c in df]\n",
    "    \n",
    "    result = pd.concat([features,df,dailyVol, labels], axis = 1).dropna()\n",
    "    preprocessed_path= os.path.join(path_to_project, 'preprocessed_data', ''.join(ticker.split('/')) + '_feat.csv')\n",
    "    result.to_csv(preprocessed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Now as we have all features ready we can start experimenting with machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
